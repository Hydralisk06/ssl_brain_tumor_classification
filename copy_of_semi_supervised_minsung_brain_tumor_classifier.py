# -*- coding: utf-8 -*-
"""Copy of semi-supervised_minsung_brain_tumor_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14wvSwNCUbgHZGE0ivYlLy_NkZnYG8Ufo
"""

!pip install semilearn
!nvidia-smi

!pip install numpy==1.24.1

import numpy as np
from torchvision import transforms
import semilearn
from semilearn import get_data_loader, get_net_builder, get_algorithm, get_config, Trainer
from semilearn import split_ssl_data, BasicDataset

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torchvision.utils import make_grid
import os
import random
import numpy as np
import pandas as pd
import pickle
import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, jaccard_score
from google.colab import drive

drive.mount('/content/drive')

torch.cuda.empty_cache()

class BrainTumorDataset(Dataset):
    def __init__(self, images, labels=None, is_labeled=False):
        self.X = images
        self.y = labels
        self.is_labeled = is_labeled

        # Assume all the transformations are as you defined earlier in your code
        self.transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])
        self.transform_augment = transforms.Compose([
            transforms.ToPILImage(),
            transforms.RandomChoice([
                transforms.RandomRotation(45),
                transforms.RandomRotation(90),
                transforms.RandomRotation(120),
                transforms.RandomRotation(180),
                transforms.RandomRotation(270),
                transforms.RandomRotation(300),
                transforms.RandomRotation(330)
            ]),
            transforms.ToTensor()
        ])

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        # Apply the transformations to the image
        if self.is_labeled:
            image = self.transform(self.X[idx])
            label = self.y[idx]
            return image, label
        else:
            # For unlabeled data, apply augmented transformation for consistency
            image = self.transform_augment(self.X[idx])
            return image, torch.tensor(-1)  # Use -1 for unlabeled data

training_data = pickle.load(open('/content/drive/My Drive/Colab Notebooks/new_dataset/training_data.pickle', 'rb'))

Xt = []
yt = []
features = None
labels = None
label = []

for features,labels in training_data:
  Xt.append(features)
  yt.append(labels)

# define configs and create config
config = {
    'algorithm': 'fixmatch',
    # 'net': 'vit_tiny_patch2_32',
    'net': 'resnet50',
    'use_pretrain': True,
    # 'pretrain_path': 'https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth',
     'pretrain_path': 'https://download.pytorch.org/models/resnet50-0676ba61.pth' ,

    # optimization configs
    'epoch': 50,  # set to 100
    'num_train_iter': 5000,  # set to 102400
    'num_eval_iter': 1000,   # set to 1024
    'num_log_iter': 50,    # set to 256
    'optim': 'AdamW',
    'lr': 5e-4,
    'layer_decay': 0.5,
    'batch_size': 16,
    'eval_batch_size': 16,
    # dataset configs
    # 'dataset': 'brain_tumor',
    # 'num_labels': 3064,
    'num_classes': 4,
    'img_size': 512,
    'crop_ratio': 0.875,
    'data_dir': './data',

    # algorithm specific configs
    'hard_label': True,
    'uratio': 2,
    'ulb_loss_ratio': 1.0,

    # device configs
    'gpu': 0,
    'world_size': 1,
    "num_workers": 2,
    'distributed': False,
}
config = get_config(config)

# create model and specify algorithm
algorithm = get_algorithm(config,  get_net_builder(config.net, from_name=False), tb_log=None, logger=None)

# 70 % training, 15% validating, 15% testing
X_train, X_test, y_train, y_test = train_test_split(Xt, yt, test_size=0.3, shuffle=True)  # 70% training, 30% testing
X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, shuffle=True)  # split testing set into 50% validation , 50% testing

Xt = None
yt = None
features = None
labels = None
label = None
training_data = None

train_set = BrainTumorDataset(X_train, y_train)
valid_set = BrainTumorDataset(X_valid, y_valid)
test_set = BrainTumorDataset(X_test, y_test)

print(f"Number of training samples: {len(X_train)}")
print(f"Number of validation samples: {len(X_valid)}")
print(f"Number of testing samples: {len(X_test)}")

print(f"Number of augmented training samples: {len(X_train) * 8}")
print(f"Number of augmented validation samples: {len(X_valid)* 8}")
print(f"Number of augmented testing samples: {len(X_test)* 8}")

print(np.array(X_train).shape)

data = np.array(X_train)
data = np.uint8(data)
print(data.shape)
target = np.array(y_train)
print(target.shape)
lb_data, lb_target, ulb_data, ulb_target = split_ssl_data(config, data, target, 10,
                                                          config.num_labels, include_lb_to_ulb=config.include_lb_to_ulb)

train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),
                                      transforms.ToTensor(),
                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

train_strong_transform = transforms.Compose([transforms.RandomHorizontalFlip(),
                                             transforms.RandAugment(),
                                             transforms.RandAugment(),
                                             transforms.AugMix(),
                                             transforms.AugMix(),
                                             transforms.ToTensor(),
                                             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

lb_dataset = BasicDataset(config.algorithm, lb_data, lb_target, config.num_classes, train_transform, is_ulb=False)
ulb_dataset = BasicDataset(config.algorithm, lb_data, lb_target, config.num_classes, train_transform, is_ulb=True, strong_transform=train_strong_transform)

eval_data = np.array(X_test)
eval_data = np.uint8(eval_data)
eval_target = np.array(y_test)

eval_transform = transforms.Compose([
                                      transforms.ToTensor(),
                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

eval_dataset = BasicDataset(config.algorithm, lb_data, lb_target, config.num_classes, eval_transform, is_ulb=False)

# define data loaders
train_lb_loader = get_data_loader(config, lb_dataset, config.batch_size)
train_ulb_loader = get_data_loader(config, ulb_dataset, int(config.batch_size * config.uratio))
eval_loader = get_data_loader(config, eval_dataset, config.eval_batch_size)

# training and evaluation
trainer = Trainer(config, algorithm)
trainer.fit(train_lb_loader, train_ulb_loader, eval_loader)
trainer.evaluate(eval_loader)